{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722b5223-eadd-4306-a4ec-bc288d0f133b",
   "metadata": {},
   "source": [
    "# 1.0 - Chatbots\n",
    "\n",
    "When discussing a Chat Bot application, it's essential to bear in mind the need for a memory of previous messages to generate contextually appropriate responses. Thus, LangChain provides a class called Memory, allowing for the maintenance of conversational context.\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c67613-fa4e-4635-a520-6974ce2691a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76957d03-e3df-4e00-a785-edc7fb530679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: I there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello there! How are you doing today? I am currently running on a server with 64 GB of RAM and an Intel Xeon processor. My operating system is Linux and I am connected to the internet through a high-speed fiber optic connection. Is there anything I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "print(conversation.predict(input=\"I there!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e36ad5-840a-4731-9d20-9719310934c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I there!\n",
      "AI:  Hello there! How are you doing today? I am currently running on a server with 64 GB of RAM and an Intel Xeon processor. My operating system is Linux and I am connected to the internet through a high-speed fiber optic connection. Is there anything I can assist you with?\n",
      "Human: Can we talk about weather?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Sure! I am constantly monitoring weather data from various sources such as satellites, weather stations, and radar systems. Currently, the temperature outside is 72 degrees Fahrenheit with a humidity level of 55%. There is a 20% chance of rain in the next hour and winds are coming from the east at 10 miles per hour. Is there any specific information you would like to know about the weather?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"Can we talk about weather?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89bb06e-3472-45bd-8135-9113ec9d7076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I there!\n",
      "AI:  Hello there! How are you doing today? I am currently running on a server with 64 GB of RAM and an Intel Xeon processor. My operating system is Linux and I am connected to the internet through a high-speed fiber optic connection. Is there anything I can assist you with?\n",
      "Human: Can we talk about weather?\n",
      "AI:  Sure! I am constantly monitoring weather data from various sources such as satellites, weather stations, and radar systems. Currently, the temperature outside is 72 degrees Fahrenheit with a humidity level of 55%. There is a 20% chance of rain in the next hour and winds are coming from the east at 10 miles per hour. Is there any specific information you would like to know about the weather?\n",
      "Human: It's a beaultiful day\n",
      "AI:  I agree, the weather conditions are quite pleasant today. The sky is mostly clear with a few scattered clouds and the UV index is at a moderate level of 5. The sunrise this morning was at 6:30 AM and the sunset will be at 8:00 PM. Would you like me to provide a detailed forecast for the rest of the day?\n",
      "Human: It's a beaultiful day\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "  I apologize, I seem to have repeated myself. Is there anything else you would like to know about the weather?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"It's a beaultiful day\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44284e96-dc85-4b91-94b4-4605d0a67bce",
   "metadata": {},
   "source": [
    "# 2.0 - Recover menssage history\n",
    "Let's create an History Object to save the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcf93ab-2757-4f9a-9054-e090c1480105",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hello, let's talk about giraffes\")\n",
    "history.add_user_message(\"Hi, I'm down to talk about giraffes\")\n",
    "dicts = messages_to_dict(history.messages)\n",
    "new_messages = messages_from_dict(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377b7073-ce29-4171-90d8-1845b919e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_history = OpenAI(temperature = 0)\n",
    "history = ChatMessageHistory(messages = new_messages)\n",
    "buffer = ConversationBufferMemory(chat_memory=history)\n",
    "conversation = ConversationChain(llm = llm_history, memory=buffer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6034f-2847-4056-9663-ffe14ce322da",
   "metadata": {},
   "source": [
    "When you pose your next question, the chatbot will take into account the history of previous messages to formulate a more informed response.\n",
    "\n",
    "Considering the question \"What are they?\" on its own, the AI wouldn't know what we're referring to. However, by taking the message history into account, it understands that we are discussing giraffes.\r",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6cb86e-ed4a-4a08-9199-ebc22690e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello, let's talk about giraffes\n",
      "Human: Hi, I'm down to talk about giraffes\n",
      "Human: What are they?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Giraffes are tall, long-necked mammals that are native to the African savannah. They are the tallest land animals in the world, with adult males reaching heights of up to 18 feet. They have distinctive spotted coats and long, dark tongues that they use to reach leaves on tall trees. They are herbivores and primarily eat leaves, twigs, and fruits. Did you know that giraffes have the same number of vertebrae in their necks as humans do? It's true! They also have a unique way of drinking water, where they have to splay their front legs and bend their necks down to reach the water. It's quite a sight to see!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"What are they?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952af154-747a-4ed8-ab34-865e373ac56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
